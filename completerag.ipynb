{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3310d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notebook is working!\")\n",
    "\n",
    "!pip install langchain chromadb openai pypdf transformers sentence-transformers torch\n",
    "\n",
    "# Import and setup\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299cc841",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PDF Ingestion + Chunking Script with Enhanced Cleaning\n",
    "------------------------------------------------------\n",
    "- Loads all PDFs from data directory\n",
    "- Cleans content to remove bibliography/references/author lists\n",
    "- Splits text into meaningful chunks\n",
    "\"\"\"\n",
    "\n",
    "def enhanced_clean_content(text):\n",
    "    \"\"\"More aggressive cleaning to remove non-research content\"\"\"\n",
    "    # Remove reference sections\n",
    "    reference_patterns = [\n",
    "        r'REFERENCES.*',\n",
    "        r'BIBLIOGRAPHY.*', \n",
    "        r'Reference[s]?.*',\n",
    "        r'Works Cited.*',\n",
    "        r'Literature Cited.*',\n",
    "    ]\n",
    "    \n",
    "    for pattern in reference_patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    # Remove citation lines and author lists\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line_clean = line.strip()\n",
    "        \n",
    "        # Skip lines that are likely citations/author lists\n",
    "        if (re.match(r'^\\[\\w+\\d{2,4}\\]', line_clean) or\n",
    "            re.match(r'^\\w+ et al\\.', line_clean) or\n",
    "            re.match(r'^\\w+, \\w+\\.', line_clean) or\n",
    "            len(line_clean.split()) > 8 and not line_clean.endswith('.') and len(line_clean) < 200 or\n",
    "            len(line_clean) < 40):\n",
    "            continue\n",
    "            \n",
    "        # Keep substantial content\n",
    "        if len(line_clean) > 50:\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "def ingest_and_clean_pdfs(pdf_dir: str):\n",
    "    print(f\"Looking for PDFs in: {pdf_dir}\")\n",
    "\n",
    "    if not os.path.exists(pdf_dir):\n",
    "        raise FileNotFoundError(f\"Directory not found: {pdf_dir}\")\n",
    "\n",
    "    # Get all PDF files\n",
    "    pdf_files = [\n",
    "        os.path.join(pdf_dir, f)\n",
    "        for f in os.listdir(pdf_dir)\n",
    "        if f.lower().endswith(\".pdf\")\n",
    "    ]\n",
    "\n",
    "    print(f\" Found {len(pdf_files)} PDF files.\\n\")\n",
    "    if not pdf_files:\n",
    "        print(\" No PDF files found. Please add PDFs to the data directory.\")\n",
    "        return [], []\n",
    "\n",
    "    all_docs = []\n",
    "    failed_files = []\n",
    "\n",
    "    # Load and clean each PDF\n",
    "    for path in pdf_files:\n",
    "        file_name = os.path.basename(path)\n",
    "        try:\n",
    "            loader = PyPDFLoader(path)\n",
    "            docs = loader.load()\n",
    "            \n",
    "            # Clean each document\n",
    "            clean_docs = []\n",
    "            for doc in docs:\n",
    "                clean_content = enhanced_clean_content(doc.page_content)\n",
    "                if len(clean_content.strip()) > 200:  # Only keep substantial content\n",
    "                    doc.page_content = clean_content\n",
    "                    clean_docs.append(doc)\n",
    "            \n",
    "            all_docs.extend(clean_docs)\n",
    "            print(f\" {file_name}: Loaded {len(clean_docs)} clean pages (from {len(docs)} original)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" {file_name}: Skipped â€” {type(e).__name__}: {e}\")\n",
    "            failed_files.append(file_name)\n",
    "\n",
    "    print(f\"\\n Total CLEAN pages loaded: {len(all_docs)}\")\n",
    "    if failed_files:\n",
    "        print(f\"Skipped {len(failed_files)} corrupted files: {failed_files}\")\n",
    "\n",
    "    # Split documents into chunks\n",
    "    print(\"\\n Splitting documents into chunks...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=150,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_documents(all_docs)\n",
    "    print(f\" Total CLEAN chunks created: {len(chunks)}\")\n",
    "\n",
    "    return all_docs, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f561ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute PDF ingestion\n",
    "BASE_DIR = os.getcwd()\n",
    "PDF_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "PDF_DIR = os.path.normpath(PDF_DIR)\n",
    "\n",
    "all_docs, chunks = ingest_and_clean_pdfs(PDF_DIR)\n",
    "print(\"\\n PDF Ingestion completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e307f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vector Database with Cleaned Content\n",
    "print(\"\\n Creating vector database with CLEANED content...\")\n",
    "\n",
    "persist_dir = \"data/chroma_db\"\n",
    "os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "# Initialize embedding model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "# Create vector store\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "print(f\" Clean Vector DB created at: {persist_dir}\")\n",
    "print(f\" Number of clean vectors: {vectordb._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee003a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Cleaned Content\n",
    "print(\"\\nðŸ” SAMPLE OF CLEANED CONTENT:\")\n",
    "sample_docs = vectordb.similarity_search(\"machine learning\", k=2)\n",
    "\n",
    "for i, doc in enumerate(sample_docs):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"Source: {os.path.basename(doc.metadata.get('source', 'Unknown'))}\")\n",
    "    print(f\"Content: {doc.page_content[:600]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d632dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "#  Enhanced RAG Query System (with deeper retrieval)\n",
    "# -----------------------------------------------\n",
    "print(\"\\nðŸ¤– Setting up Enhanced RAG Query System...\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize text generator\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    max_length=300,\n",
    "    temperature=0.3,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "def smart_rag_query(question, num_docs=8):  #  Increased from 4 â†’ 8 (Option 2)\n",
    "    \"\"\"\n",
    "    Enhanced RAG with better context handling and deeper document retrieval.\n",
    "    \"\"\"\n",
    "    # ðŸ” Retrieve more relevant documents (Option 2)\n",
    "    docs = vectordb.similarity_search(question, k=num_docs)\n",
    "    \n",
    "    if not docs:\n",
    "        return {\n",
    "            \"answer\": \"No relevant documents found.\",\n",
    "            \"sources\": []\n",
    "        }\n",
    "\n",
    "    #  Build clean context from top documents\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        content = doc.page_content\n",
    "        \n",
    "        # Clean the content for clarity\n",
    "        lines = content.split('\\n')\n",
    "        clean_lines = [line.strip() for line in lines if len(line.strip()) > 30]\n",
    "        clean_content = ' '.join(clean_lines[:5])  # Take top 5 meaningful lines\n",
    "        \n",
    "        if clean_content:\n",
    "            context_parts.append(f\"Document {i+1}:\\n{clean_content}\")\n",
    "\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # Create an optimized, synthesis-focused prompt\n",
    "    prompt = f\"\"\"You are an expert research assistant.\n",
    "Using the following context from multiple academic documents, answer the question clearly.\n",
    "\n",
    "RESEARCH CONTENT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Please provide a clear and concise explanation based only on the context above:\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Generate a summarized answer from all sources\n",
    "        response = generator(prompt, max_length=250)[0]['generated_text']\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.strip(),\n",
    "            \"sources\": docs\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"answer\": f\"Error generating response: {str(e)}\",\n",
    "            \"sources\": docs\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23467f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG System with Multiple Queries\n",
    "print(\"\\n TESTING RAG SYSTEM\")\n",
    "\n",
    "test_queries = [\n",
    "    \"What are the main machine learning techniques discussed?\",\n",
    "    \"What problems in artificial intelligence are being addressed?\",\n",
    "    \"What are the key findings about neural networks?\",\n",
    "    \"What optimization methods are mentioned for deep learning?\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TEST {i}: {query}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    result = smart_rag_query(query)\n",
    "    \n",
    "    print(\" ANSWER:\")\n",
    "    print(result[\"answer\"])\n",
    "    \n",
    "    if result[\"sources\"]:\n",
    "        print(f\"\\n SOURCES ({len(result['sources'])} documents):\")\n",
    "        unique_sources = set()\n",
    "        for doc in result[\"sources\"]:\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            filename = os.path.basename(source)\n",
    "            if filename not in unique_sources:\n",
    "                unique_sources.add(filename)\n",
    "                print(f\"  - {filename}\")\n",
    "    \n",
    "    print(f\"\\n Response generated successfully!\")\n",
    "\n",
    "print(f\"\\n RAG SYSTEM READY! Total documents in database: {vectordb._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26334ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Query System\n",
    "print(\"\\n INTERACTIVE QUERY SYSTEM\")\n",
    "print(\"Type 'quit' to exit, 'sources' to see database info\\n\")\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"\\n Your question: \").strip()\n",
    "    \n",
    "    if user_query.lower() == 'quit':\n",
    "        print(\" Goodbye!\")\n",
    "        break\n",
    "    elif user_query.lower() == 'sources':\n",
    "        print(f\" Database Info:\")\n",
    "        print(f\"   - Total vectors: {vectordb._collection.count()}\")\n",
    "        print(f\"   - Location: {persist_dir}\")\n",
    "        continue\n",
    "    elif not user_query:\n",
    "        continue\n",
    "    \n",
    "    print(\" Processing...\")\n",
    "    result = smart_rag_query(user_query)\n",
    "    \n",
    "    print(f\"\\nANSWER:\")\n",
    "    print(result[\"answer\"])\n",
    "    \n",
    "    if result[\"sources\"]:\n",
    "        print(f\"\\n Based on {len(result['sources'])} sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757fa83a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
